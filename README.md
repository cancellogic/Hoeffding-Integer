# Integer-Hoeffding
This is Wassley Hoeffding's 1948 equation for detecting -frequently nonlinear- relationships in data or variables.  In this version, the association value is computed with integers calculation, and not normalized (please only compare same n sample sizes) or shaped to a probability (higher values do have stronger relationships- good for detection of better models in machine learning or genetic algorithm health assessment -- but you won't know if a +81 difference is a miserable or vast improvement). Coded in Rust for partially ordinal types (compare {"g","e","n","e","r","i","c","s"} and {3.0,1.0,4.0,1.0,5.0,4.0,3.5,6.1})
# In defense of comprimises:
**I didn't compute the whole function.**  Roughly normalize the result if you want with division by ((n*(n-1)*(n-2)*(n-3)*(n-4))/30).  In my estimate, that undermines genetic algorthim health functions that might evolve towards models that return high normalized correlation despite being largely impossible to compute (infities, undefined, not a number, overflow, underflow)--  in a 'works great until it works against you' kind of way.  Admitedly, my thinking that Hoeffding normalization for GA is problematic is niche and not a formal mathmatical proof. 

**Generic functions like this one can cause slow compiles as n*n versions are made for n (every) partial ordinal type.**  For the data miner, it isn't that unusual to compare integers and floats to characters or booleans, so this algorithm can do that, while most published Hoeffding discriminators are type 'apples to apples' constrained.    

**No probability is reported.**  There seem to be about 20+ papers published in the last 60 years in this area... many claiming easy, simplified, no need for an infinite chain of perfectly random bell curve distributed data points to define the probability function for each n because Eugen Slutsky's theorm or Lorenz attractors for even odd n converge to 3 significant digits (99.9%) or something.  I didn't read more than abstracts for most.  And I understand enough of the math to know I should stop here, with unanswered questions and knowledge that someone with a post doctorate degree in number theory or statistics would probably be needed to take this further.   

**Faster?**  Rust is pretty fast.  Integer math is pretty fast.  This code is hopefully, fast enough for your needs - but sorting uses a lot of time in Hoeffding rankings for his correlation function.  If you are short on time, short on compute power, or certain you have a linear model - please strongly consider Pearson's R correlation (maybe crates.io/crates/ndarray-stats).  Pearsons favors linear models and isn't ideal for nonlinear GA goal seeking, but situationally, for many nonlinear models, Pearson's R correlation is good enough. 

**Am I doing the math correctly** despite obvious intended scaling/multiplication to avoid fractions and floating points?  I'm not certain of correctness, I'm outside of my expertise, and I'm not content with my own work.  If this was computing a mean, standard deviation or Pearsons R - no problem. But I do find Hoeffding's ideas beautiful and difficult.  Beware the ⇴ unicode21F4 ⇴ arrow notation, it appears to mean "Hoeffding leaves this partially defined probability function to be worked on by dozens of others for seven decades."  For me, perhaps even being wrong is still useful.
